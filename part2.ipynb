{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: VAE for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114f6aed0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.distributions\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\"  # for Apple M-series CPUs\n",
    "TRAIN_DATASET_DIR = \"dataset/train\"\n",
    "NEGATIVE_DATASET_DIR = \"dataset/proliv\"\n",
    "TEST_DATASET_DIR = \"dataset/test/imgs\"\n",
    "TEST_DATASET_LABELS_PATH = \"dataset/test/test_annotation.txt\"\n",
    "FIXED_IMG_SIZE = (30, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(4500, 512)\n",
    "        self.linear2 = nn.Linear(512, latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dims, 512)\n",
    "        self.linear2 = nn.Linear(512, 4500)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.linear1(z))\n",
    "        z = torch.sigmoid(self.linear2(z))\n",
    "        return z.reshape((-1, 3, *FIXED_IMG_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset(Dataset):\n",
    "    def __init__(self, dataset_path, transform=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform\n",
    "        self.data = self.__load_path()\n",
    "\n",
    "    def __load_path(self):\n",
    "        data = [\n",
    "            os.path.join(self.dataset_path, el)\n",
    "            for el in os.listdir(self.dataset_path)\n",
    "        ]\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = read_image(self.data[idx], mode=ImageReadMode.RGB)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(autoencoder, data, epochs=20):\n",
    "    opt = torch.optim.Adam(autoencoder.parameters())\n",
    "\n",
    "    for i in range(epochs):\n",
    "        l = []\n",
    "        for x in data:\n",
    "            x = x.to(DEVICE)\n",
    "            x_hat = autoencoder(x)\n",
    "\n",
    "            loss = ((x - x_hat) ** 2).sum()\n",
    "            loss.backward()\n",
    "\n",
    "            l.append(loss.item())\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        print(f\"loss for 1 img on ep {i + 1} = {np.mean(l) / len(x)}\")\n",
    "            \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 256\n",
    "autoencoder = Autoencoder(latent_dims).to(DEVICE)  # GPU\n",
    "autoencoder.train()\n",
    "\n",
    "data = LoadDataset(\n",
    "    TRAIN_DATASET_DIR,\n",
    "    transforms.Compose([transforms.Resize(FIXED_IMG_SIZE)]),\n",
    ")\n",
    "dataset = torch.utils.data.DataLoader(data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 1 img on ep 1 = 29.28859496875933\n",
      "loss for 1 img on ep 2 = 23.557213929048768\n",
      "loss for 1 img on ep 3 = 19.339917362115944\n",
      "loss for 1 img on ep 4 = 16.489770108727132\n",
      "loss for 1 img on ep 5 = 13.734824815373512\n",
      "loss for 1 img on ep 6 = 11.514012537184794\n",
      "loss for 1 img on ep 7 = 11.334676830631912\n",
      "loss for 1 img on ep 8 = 9.571553649416394\n",
      "loss for 1 img on ep 9 = 9.088259334017517\n",
      "loss for 1 img on ep 10 = 8.59425008676614\n",
      "loss for 1 img on ep 11 = 9.88842265241465\n",
      "loss for 1 img on ep 12 = 8.135554624211256\n",
      "loss for 1 img on ep 13 = 7.941578417067316\n",
      "loss for 1 img on ep 14 = 7.790046069272764\n",
      "loss for 1 img on ep 15 = 7.945160118637571\n",
      "loss for 1 img on ep 16 = 8.627753878854643\n",
      "loss for 1 img on ep 17 = 7.075148491343116\n",
      "loss for 1 img on ep 18 = 7.249231733334292\n",
      "loss for 1 img on ep 19 = 7.52097660720728\n",
      "loss for 1 img on ep 20 = 6.597087937555496\n",
      "loss for 1 img on ep 21 = 6.7363345835618915\n",
      "loss for 1 img on ep 22 = 6.6828614936512745\n",
      "loss for 1 img on ep 23 = 6.578720580240724\n",
      "loss for 1 img on ep 24 = 6.527296166511098\n",
      "loss for 1 img on ep 25 = 6.216274310069479\n",
      "loss for 1 img on ep 26 = 6.54286995663005\n",
      "loss for 1 img on ep 27 = 6.833780659991465\n",
      "loss for 1 img on ep 28 = 5.831011469956416\n",
      "loss for 1 img on ep 29 = 6.072503975242566\n",
      "loss for 1 img on ep 30 = 6.346349978902538\n",
      "loss for 1 img on ep 31 = 5.816772262002252\n",
      "loss for 1 img on ep 32 = 5.836257048473237\n",
      "loss for 1 img on ep 33 = 5.88363738880036\n",
      "loss for 1 img on ep 34 = 5.975647684874808\n",
      "loss for 1 img on ep 35 = 5.6612508388081935\n",
      "loss for 1 img on ep 36 = 5.714325108345906\n",
      "loss for 1 img on ep 37 = 5.702669565844688\n",
      "loss for 1 img on ep 38 = 6.901733758343253\n",
      "loss for 1 img on ep 39 = 5.281092756113429\n",
      "loss for 1 img on ep 40 = 6.7544947733545\n",
      "loss for 1 img on ep 41 = 5.53776155611512\n",
      "loss for 1 img on ep 42 = 5.244773278570479\n",
      "loss for 1 img on ep 43 = 5.683736265085305\n",
      "loss for 1 img on ep 44 = 5.3732644554915705\n",
      "loss for 1 img on ep 45 = 5.803109009554432\n",
      "loss for 1 img on ep 46 = 5.628370088376816\n",
      "loss for 1 img on ep 47 = 5.130588219423962\n",
      "loss for 1 img on ep 48 = 5.319587892787472\n",
      "loss for 1 img on ep 49 = 5.426970077927705\n",
      "loss for 1 img on ep 50 = 5.487257849638629\n"
     ]
    }
   ],
   "source": [
    "autoencoder_1 = train(autoencoder, dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We got loss~5.5 for positive imgs, let's calculate stats for negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for negative sample 1: 19.744293212890625\n",
      "loss for negative sample 2: 22.14520263671875\n",
      "loss for negative sample 3: 18.16210174560547\n",
      "loss for negative sample 4: 19.1495304107666\n",
      "loss for negative sample 5: 35.30286407470703\n",
      "loss for negative sample 6: 19.652511596679688\n",
      "loss for negative sample 7: 15.875557899475098\n",
      "loss for negative sample 8: 15.68686294555664\n",
      "loss for negative sample 9: 18.968942642211914\n",
      "loss for negative sample 10: 12.721654891967773\n",
      "loss for negative sample 11: 40.552734375\n",
      "loss for negative sample 12: 21.443111419677734\n",
      "loss for negative sample 13: 22.695350646972656\n",
      "loss for negative sample 14: 15.401663780212402\n",
      "loss for negative sample 15: 26.621822357177734\n",
      "loss for negative sample 16: 28.74909782409668\n",
      "loss for negative sample 17: 15.184796333312988\n",
      "loss for negative sample 18: 14.713872909545898\n",
      "loss for negative sample 19: 18.370807647705078\n",
      "loss for negative sample 20: 12.375375747680664\n",
      "loss for negative sample 21: 29.155614852905273\n",
      "loss for negative sample 22: 18.40665054321289\n",
      "loss for negative sample 23: 21.11707305908203\n",
      "loss for negative sample 24: 22.73444938659668\n",
      "loss for negative sample 25: 19.588302612304688\n",
      "loss for negative sample 26: 27.228565216064453\n",
      "loss for negative sample 27: 18.903871536254883\n",
      "loss for negative sample 28: 15.944047927856445\n",
      "loss for negative sample 29: 16.98506736755371\n",
      "loss for negative sample 30: 17.588464736938477\n",
      "loss for negative sample 31: 32.57195281982422\n",
      "loss for negative sample 32: 15.219888687133789\n",
      "loss for negative sample 33: 16.889328002929688\n",
      "loss for negative sample 34: 16.766372680664062\n",
      "loss for negative sample 35: 22.882648468017578\n",
      "loss for negative sample 36: 16.03260040283203\n",
      "loss for negative sample 37: 16.862041473388672\n",
      "loss for negative sample 38: 17.44997215270996\n",
      "loss for negative sample 39: 24.844926834106445\n",
      "loss for negative sample 40: 17.460002899169922\n",
      "loss for negative sample 41: 17.480140686035156\n",
      "loss for negative sample 42: 27.370607376098633\n",
      "loss for negative sample 43: 31.134510040283203\n",
      "loss for negative sample 44: 19.588302612304688\n",
      "loss for negative sample 45: 21.732166290283203\n",
      "loss for negative sample 46: 31.184356689453125\n",
      "loss for negative sample 47: 19.652511596679688\n",
      "loss for negative sample 48: 18.770275115966797\n",
      "loss for negative sample 49: 19.744293212890625\n",
      "loss for negative sample 50: 15.626446723937988\n",
      "loss for negative sample 51: 31.259510040283203\n",
      "loss for negative sample 52: 17.588464736938477\n",
      "loss for negative sample 53: 23.776782989501953\n",
      "loss for negative sample 54: 14.177515029907227\n",
      "loss for negative sample 55: 17.23649787902832\n",
      "loss for negative sample 56: 24.491987228393555\n",
      "loss for negative sample 57: 22.695350646972656\n",
      "loss for negative sample 58: 14.547796249389648\n",
      "loss for negative sample 59: 28.492340087890625\n",
      "loss for negative sample 60: 17.460002899169922\n",
      "loss for negative sample 61: 17.59550666809082\n",
      "loss for negative sample 62: 15.944047927856445\n",
      "loss for negative sample 63: 17.470802307128906\n",
      "loss for negative sample 64: 14.878402709960938\n",
      "loss for negative sample 65: 12.51873779296875\n",
      "loss for negative sample 66: 9.853157043457031\n",
      "loss for negative sample 67: 21.12078857421875\n",
      "loss for negative sample 68: 19.156368255615234\n",
      "loss for negative sample 69: 25.53217887878418\n",
      "loss for negative sample 70: 19.327939987182617\n",
      "loss for negative sample 71: 14.860700607299805\n",
      "loss for negative sample 72: 12.350691795349121\n",
      "loss for negative sample 73: 17.525814056396484\n",
      "loss for negative sample 74: 15.668439865112305\n",
      "loss for negative sample 75: 15.35661506652832\n",
      "loss for negative sample 76: 23.796459197998047\n",
      "loss for negative sample 77: 16.22226333618164\n",
      "loss for negative sample 78: 11.31005859375\n",
      "loss for negative sample 79: 28.374902725219727\n",
      "loss for negative sample 80: 11.31005859375\n",
      "loss for negative sample 81: 17.23649787902832\n",
      "loss for negative sample 82: 27.228565216064453\n",
      "loss for negative sample 83: 14.517024040222168\n",
      "loss for negative sample 84: 19.156368255615234\n",
      "loss for negative sample 85: 16.03260040283203\n",
      "loss for negative sample 86: 25.29825210571289\n",
      "loss for negative sample 87: 16.766372680664062\n",
      "loss for negative sample 88: 17.181669235229492\n",
      "loss for negative sample 89: 31.184356689453125\n",
      "loss for negative sample 90: 20.986839294433594\n",
      "loss for negative sample 91: 20.91141128540039\n",
      "loss for negative sample 92: 16.96755599975586\n",
      "loss for negative sample 93: 18.430622100830078\n",
      "loss for negative sample 94: 23.675159454345703\n",
      "loss for negative sample 95: 16.0463809967041\n",
      "loss for negative sample 96: 18.11769676208496\n",
      "loss for negative sample 97: 15.183294296264648\n",
      "loss for negative sample 98: 31.989206314086914\n",
      "loss for negative sample 99: 14.59614372253418\n",
      "loss for negative sample 100: 14.169658660888672\n",
      "loss for negative sample 101: 16.331064224243164\n",
      "loss for negative sample 102: 12.721654891967773\n",
      "loss for negative sample 103: 25.369112014770508\n",
      "loss for negative sample 104: 28.561323165893555\n",
      "loss for negative sample 105: 18.40665054321289\n",
      "loss for negative sample 106: 17.45625877380371\n",
      "loss for negative sample 107: 16.599224090576172\n",
      "loss for negative sample 108: 23.393173217773438\n",
      "loss for negative sample 109: 15.123300552368164\n",
      "loss for negative sample 110: 22.71723175048828\n",
      "loss for negative sample 111: 20.44744110107422\n",
      "loss for negative sample 112: 19.71283531188965\n",
      "loss for negative sample 113: 17.181669235229492\n",
      "loss for negative sample 114: 12.178719520568848\n",
      "loss for negative sample 115: 16.89470672607422\n",
      "loss for negative sample 116: 22.343074798583984\n",
      "loss for negative sample 117: 23.304828643798828\n",
      "loss for negative sample 118: 15.401663780212402\n",
      "loss for negative sample 119: 17.29550552368164\n",
      "loss for negative sample 120: 17.45625877380371\n",
      "loss for negative sample 121: 17.121667861938477\n",
      "loss for negative sample 122: 31.989206314086914\n",
      "loss for negative sample 123: 25.75360870361328\n",
      "loss for negative sample 124: 17.525814056396484\n",
      "loss for negative sample 125: 14.464096069335938\n",
      "loss for negative sample 126: 23.78668975830078\n",
      "loss for negative sample 127: 16.889328002929688\n",
      "loss for negative sample 128: 21.460567474365234\n",
      "loss for negative sample 129: 31.184356689453125\n",
      "loss for negative sample 130: 27.228565216064453\n",
      "loss for negative sample 131: 15.401663780212402\n",
      "loss for negative sample 132: 17.45625877380371\n",
      "loss for negative sample 133: 14.702886581420898\n",
      "loss for negative sample 134: 19.156368255615234\n",
      "loss for negative sample 135: 15.602588653564453\n",
      "loss for negative sample 136: 15.393390655517578\n",
      "loss for negative sample 137: 16.395334243774414\n",
      "loss for negative sample 138: 19.327939987182617\n",
      "loss for negative sample 139: 25.220918655395508\n",
      "loss for negative sample 140: 16.22226333618164\n",
      "loss for negative sample 141: 18.988691329956055\n",
      "loss for negative sample 142: 15.626446723937988\n",
      "loss for negative sample 143: 16.96755599975586\n",
      "loss for negative sample 144: 23.675159454345703\n",
      "loss for negative sample 145: 29.155614852905273\n",
      "loss for negative sample 146: 25.007728576660156\n",
      "loss for negative sample 147: 12.721654891967773\n",
      "loss for negative sample 148: 15.437080383300781\n",
      "loss for negative sample 149: 30.02939224243164\n",
      "loss for negative sample 150: 14.841032981872559\n",
      "loss for negative sample 151: 17.23649787902832\n",
      "loss for negative sample 152: 11.31005859375\n",
      "loss for negative sample 153: 35.367733001708984\n",
      "loss for negative sample 154: 17.181669235229492\n",
      "min loss = 9.853157043457031\n",
      "mean loss = 19.82295448129827\n"
     ]
    }
   ],
   "source": [
    "data_val = LoadDataset(\n",
    "    NEGATIVE_DATASET_DIR,\n",
    "    transforms.Compose([transforms.Resize(FIXED_IMG_SIZE)]),\n",
    ")\n",
    "dataset_val = torch.utils.data.DataLoader(data, batch_size=1, shuffle=False)\n",
    "\n",
    "autoencoder.eval()\n",
    "l = []\n",
    "with torch.no_grad():\n",
    "    for i, x in enumerate(data_val):\n",
    "        x = x.unsqueeze(0).to(DEVICE)\n",
    "        p = autoencoder(x)\n",
    "        l.append(((x - p) ** 2).sum().item())\n",
    "        print(f\"loss for negative sample {i + 1}: {l[-1]}\")\n",
    "\n",
    "print(f\"min loss = {min(l)}\")\n",
    "print(f\"mean loss = {np.mean(l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take threshold val = 12 and validate model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTestDataset(Dataset):\n",
    "    def __init__(self, dataset_path, annot, transform=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.annot = annot\n",
    "        self.transform = transform\n",
    "        self.data, self.labels = self.__load_path()\n",
    "\n",
    "    def __load_path(self):\n",
    "        data = [\n",
    "            os.path.join(self.dataset_path, el)\n",
    "            for el in os.listdir(self.dataset_path)\n",
    "        ]\n",
    "\n",
    "        labels = {}\n",
    "        with open(self.annot) as a:\n",
    "            for l in [el.rstrip() for el in a.readlines()]:\n",
    "                img, lab = l.split()\n",
    "                labels[img] = int(lab)\n",
    "\n",
    "        return data, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = read_image(self.data[idx], mode=ImageReadMode.RGB)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img / 255.0, self.labels[os.path.basename(self.data[idx])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR = 0.8455661664392906\n",
      "TNR = 0.8217054263565892\n"
     ]
    }
   ],
   "source": [
    "data_val = LoadTestDataset(\n",
    "    TEST_DATASET_DIR,\n",
    "    TEST_DATASET_LABELS_PATH,\n",
    "    transforms.Compose([transforms.Resize(FIXED_IMG_SIZE)]),\n",
    ")\n",
    "dataset_val = torch.utils.data.DataLoader(data, batch_size=1, shuffle=False)\n",
    "\n",
    "autoencoder.eval()\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(data_val):\n",
    "        x = x.unsqueeze(0).to(DEVICE)\n",
    "        p = autoencoder(x)\n",
    "        loss = ((x - p) ** 2).sum().item()\n",
    "\n",
    "        if loss < THRESH and not y:\n",
    "            tp += 1\n",
    "        elif loss < THRESH and y:\n",
    "            fp += 1\n",
    "        elif loss >= THRESH and not y:\n",
    "            fn += 1\n",
    "        elif loss >= THRESH and y:\n",
    "            tn += 1\n",
    "\n",
    "print(f\"TPR = {tp / (tp + fn)}\")\n",
    "print(f\"TNR = {tn / (tn + fp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So we got ~83% for TPR and TNR!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
